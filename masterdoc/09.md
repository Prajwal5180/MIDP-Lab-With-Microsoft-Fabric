# Exercise 2: Build a lakehouse

### Estimated Duration : 1 Hour 15 minutes

In this exercise, you will build a lakehouse by activating SharePoint Online, creating the lakehouse, ingesting sample data, and building a report.

## Lab objectives

- Task 1: Activate SharePoint Online
- Task 2: Create a lakehouse
- Task 3: Ingest sample data
- Task 4: Build a report

## Task 1: Activate SharePoint Online

1. To activate SharePoint Online, copy the **Office homepage link** and open this link inside the VM in a new tab.

   ```
   https://office.com
   ```

2. Click on **Sign in (1)**.

   ![01s](../media/01s.png)

3. If you see the prompt **Action Required**, click on **Ask Later (1)**.

   ![02s](../media/09/02s.png)

4. You have now successfully signed into **Office homepage** and activated **SharePoint Online**.

   ![03s](../media/09/03s.png)

   >**Note:** This will be required for a particular task in the lab.

5. You can now close this tab and proceed to the next task.

----

## Task 2: Create a lakehouse

1. From the newly created Power BI workspace, make use of the **Experience switches (1)** located at the bottom left, and select **Data Engineering (2)**.

   ![exp-switcher-data-engg](../media/09/01.png)

2. In the **Data Engineering** tab, select **Lakehouse (1)** to create a lakehouse.

   ![](../media/Fabric7.png)

3. If Prompted then click on **Upgrade** tab.

   ![](../media/lakehouse-exercise2-upgrade.png) 

4. In the **New lakehouse** dialog box, enter **wwilakehouse (1)** in the **Name** field, and click on **Create (2)** to create and open the new lakehouse.

   ![enter-wwilakehouse-name](../media/09/03.png)

> **Congratulations** on completing the task! Now, it's time to validate it. Here are the steps:
> - Hit the Validate button for the corresponding task. If you receive a success message, you can proceed to the next task. Alternatively, you can navigate to the Lab Validation Page, from the upper right corner in the lab guide section.
> - If not, carefully read the error message and retry the step, following the instructions in the lab guide. 
> - If you need any assistance, please contact us at labs-support@spektrasystems.com. We are available 24/7 to help you out.
 
   <validation step="98c90546-5680-4978-b3ae-eb32a2422fdc" />

----

## Task 3: Ingest sample data

1. In the **Lakehouse Explorer**, you see options to load data into Lakehouse. Select **New Dataflow Gen2 (1)**.

   ![select-dataflow-gen2](../media/09/04.png)

2. On the new dataflow pane, select **Import from a Text/CSV file (1)**.

   ![select-option-to-import-csv-file](../media/09/05.png)

3. On the **Connect to data source (1)** pane, select the **Upload file (Preview) (2)** radio button, and then click on **Browse (3)**.

   ![Browse](../media/09/06.png)

4. In the Open window, navigate to the **C:\FabricFiles (1)** folder, select the **dimension_customer.csv (2)** file, and click on **Open (3)**.

   ![navigate-csv](../media/09/07.png)  

5. After the file is uploaded, select **Next (1)**.

   ![selectnext](../media/09/08.png)

   >**Note:** The **Preview file data** page might require a few minutes to load.

6. From the **Preview file data** page, preview the data and select **Create (1)** to proceed and return back to the dataflow canvas.
   
   ![select-create](../media/09/09.png)

7. It will open up a **Power Query** editor. In the **Query settings (1)** pane, verify whether **dimension_customer (2)** is reflected under the **Name** field. 

   ![verify-name-and-lakehouse](../media/09/10.png)

8. Now, we will be removing the pre-configured **Data destination** and configuring it manually, click on the **X (1)** next to the Lakehouse icon under **Data destination**.

   ![click-x](../media/09/11.png)

9. It will open up a prompt which will ask you to confirm to **Remove data destination**, and click on **OK (1)**.

   ![click-ok](../media/09/12.png)
   
10. Now, in the **Query (1)** editor, click on **Add data destination (2)**, and select **Lakehouse (3)**.

    ![set-data-lakehouse-dest](../media/09/13.png)

11. In the **Connect to data destination** pane, confirm the **Connection (1)**, and click on **Next (2)**.

    ![connection-next](../media/09/14.png)

12. In the **Choose destination target** pane, confirm the name of the table to be **dimension_customer (1)**. Now, expand your workspace select **wwilakehouse (2)**, and click on **Next (3)**.

    ![data-destination-lakehouse](../media/lakehouse-exercise2-destinationtarget.png) 

14. In the **Choose destination settings**, select **Save Settings (1)** to return to the dataflow canvas.

    ![replace-as-update-method-save-settings](../media/09/16.png)

15. From the dataflow canvas, select **Publish (1)** at the bottom right of the screen.

    ![publish-canvas](../media/09/17.png)

16. A spinning circle next to the dataflow's name indicates publishing is in progress in the item view.

    ![dataflow-in-progress](../media/09/18.png)

17. Once publishing is complete, click on the **...(1)** and select **Properties (2)**.

    ![select-properties](../media/09/19.png)

18. Rename the dataflow to **Load Lakehouse Table (1)** and select **Save (2)**.

    ![](../media/09/E2(2)-T2.3-S17.png)

19. Select the **Refresh now (1)** option next to the data flow name to refresh the data flow. It runs the dataflow and moves data from the source file to the lakehouse table.

    ![refresh-data-flow](../media/09/21.png)

20. Once the dataflow is refreshed, select your lakehouse from the **Fabric workspace**.

    ![select-lakehouse](../media/09/22a.png)

21. Now, you can view the **dimension_customer (1)** delta table. Select the table to preview its data.

    >**Note**: If you are unable to view _dimension_customer_ delta table, please refresh the page.

    ![preview-customer-data](../media/09/23.png)

23. You can also use the SQL analytics endpoint of the lakehouse to query the data with SQL statements. Select **SQL analytics endpoint** from the Lakehouse drop-down menu at the top right of the screen.

    ![sql-endpoint](../media/lakehouse-exercise2-endpoint.png)  

24. Select **New SQL query (1)** to write your SQL statements.

    ![new-sql-query](../media/09/25.png)

25. Input the following SQL query which aggregates the row count based on the BuyingGroup column of the **dimension_customer** table.

    ```
    SELECT BuyingGroup, Count(*) AS Total
    FROM dimension_customer
    GROUP BY BuyingGroup
    ```

26. Click on **Run (1)** to run the query, and view the results.

    ![view-sql-query-results](../media/09/26.png)

----

## Task 4: Build a report

1. Select your lakehouse from the **Fabric workspace**.

    ![select-lakehouse](../media/09/22a.png)

2.  Open your lakehouse and switch to the **SQL analytics endpoint** view.

    ![sql-endpoint](../media/lakehouse-exercise2-endpoint.png)

3. Previously all the lakehouse tables and views were automatically added to the semantic model. With recent updates, for new lakehouses, you must manually add your tables to the semantic model. From the **Reporting (1)** tab, select **Manage default semantic model (2)** and select the tables that you want to add to the semantic model. In this case, select the **dimension_customer (5)** table under **dbo (3)** > **Tabels (4)**, Click on **Confirm (6)**

   ![](../media/09/22ab.png)

4. Click on **Automatically update semantic model**.

   ![](../media/09/22ac.png)

1. From the workspace, select the **wwilakehouse Semantic model (1)**. 

   ![](../media/09/22.png)

2. From the dataset pane, you can view all the tables. You have options to create reports either from scratch, paginated reports or let Power BI automatically create a report based on your data. For this lab, select **Explore this data (1)** and click on **Auto-create a report (2)**.

   ![](../media/Fabric9.png)

3. Since the table is a dimension and there are no measures in it, Power BI creates a measure for the row count aggregates it across different columns, and creates different charts as shown in the following image. You can save this report for the future by selecting **Save (1)** from the top ribbon.

   ![save-report](../media/09/29.png)

4. Name the Report and click on **save**.

   ![](../media/09/lakehouse-exercise2-Reporta.png)

5. Your report will now be stored within the same Power BI workspace, ensuring that all your data and visualizations remain organized in one central location for easy access and management.

## Summary

In this exercise, you have created a lakehouse, ingested sample data and built a report.

### You have successfully completed the lab >> Click on Next
